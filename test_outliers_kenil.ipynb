{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.mllib.feature import StandardScaler, StandardScalerModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.clustering import GaussianMixture, GaussianMixtureModel\n",
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import sys\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "import get_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Nulls and Outliers Detection 1\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.101.10.75:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Nulls and Outliers Detection 1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Nulls and Outliers Detection 1>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df = spark.read.csv(path = 'GROUP7/bss9-579f_clean/part-00000-26b4c5d2-1b04-44e3-aff2-769b8351c897-c000.csv', header = True,inferSchema = True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = []\n",
    "for col,dtype in df_temp.dtypes:\n",
    "    if 'string' not in dtype and col!='rid':\n",
    "        numeric_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols_temp = numeric_cols[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comparable_rental_2_gross_sqft',\n",
       " 'comparable_rental_3_estimated_gross_income']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp.select([numeric_cols[2],*numeric_cols_temp])\n",
    "df_temp = df_temp.withColumn('rid', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col_rdd = df_temp.select(['rid',*numeric_cols_temp]).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_multivariate(df, numeric_cols, k=3, maxIterations=100):\n",
    "    def addclustercols(x):\n",
    "        points = np.array(x[1].toArray()).astype(float)\n",
    "        center = clusters.centers[0]\n",
    "        mindist = euclidean(points, center)\n",
    "        c1 = 0\n",
    "\n",
    "        for i in range(1, len(clusters.centers)):\n",
    "            center = clusters.centers[i]\n",
    "            dist = euclidean(points, center)\n",
    "            if dist < mindist:\n",
    "                c1 = i\n",
    "                mindist = dist\n",
    "        return (int(x[0]), int(c1), float(mindist))\n",
    "\n",
    "    cols = ['rid']\n",
    "    cols.extend(numeric_cols)\n",
    "    df_col_rdd = df[cols].rdd\n",
    "    label = df_col_rdd.map(lambda x: x[0])\n",
    "    vso = df_col_rdd.map(lambda x: np.array(x[1:]).astype(float))\n",
    "    scaler = StandardScaler(withMean=True, withStd=True).fit(vso)\n",
    "    vso = scaler.transform(vso)\n",
    "\n",
    "    clusters = KMeans.train(vso, k, initializationMode='random', maxIterations=maxIterations)\n",
    "    df_col_rdd = label.zip(vso).toDF().rdd\n",
    "    print(df_col_rdd.collect())\n",
    "    rdd_w_clusts = df_col_rdd.map(lambda x: addclustercols(x))\n",
    "    cols = ['rid', 'c_no', 'dist_c']\n",
    "    kmeans_df = rdd_w_clusts.toDF(cols)\n",
    "    outlier_all, _ = iqr_outliers(kmeans_df.where(kmeans_df['c_no'] == 0), 'dist_c')\n",
    "    for i in range(1, k):\n",
    "        outlier_c, _ = iqr_outliers(kmeans_df.where(kmeans_df['c_no'] == i), 'dist_c')\n",
    "        outlier_all = outlier_all.unionAll(outlier_c)\n",
    "    #print_outlier_summary(outlier_all.count(), df.count(), \"kMeans (multivariate)\")\n",
    "    return outlier_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistances(x):\n",
    "    clust_center = x[0]\n",
    "    rid = x[1][0]\n",
    "    point = np.array(x[1][1].toArray()).astype(float)\n",
    "    dist = mahalanobis(clust_center,point,sigmas_inv[clust_center])\n",
    "    return (int(rid),int(clust_center),float(dist))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = df_col_rdd.map(lambda x:x[0])\n",
    "features = df_col_rdd.map(lambda x: np.array(x[1:]).astype(float))\n",
    "scaler2 = StandardScaler(withMean=True, withStd=True).fit(features)\n",
    "features = scaler2.transform(features)\n",
    "zipped_col = rid.zip(features)\n",
    "gmm = GaussianMixture.train(features,3)\n",
    "labels = gmm.predict(features)\n",
    "mus = []\n",
    "sigmas = []\n",
    "sigmas_inv = []\n",
    "for i in range(3):\n",
    "    mus.append(np.array(gmm.gaussians[i].mu.toArray()).astype(float))\n",
    "    sigmas.append(np.array(gmm.gaussians[i].sigma.toArray()).astype(float))\n",
    "    sigmas_inv.append(np.linalg.inv(sigmas[i]))\n",
    "final_rdd = labels.zip(zipped_col)\n",
    "rdd_w_clusts = final_rdd.map(lambda x: getDistances(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------------+\n",
      "|rid|c_no|            dist_c|\n",
      "+---+----+------------------+\n",
      "|  1|   0|1.3417595880225692|\n",
      "|  2|   0|0.6259355430408566|\n",
      "|  3|   0| 1.969200743054647|\n",
      "|  4|   0| 1.969200743054647|\n",
      "|  6|   0|3.4587405022484377|\n",
      "|  8|   0|1.2100683440665077|\n",
      "|  9|   0| 0.570260286659337|\n",
      "| 10|   0|1.7980582243233845|\n",
      "| 11|   0|2.5274539774592917|\n",
      "| 12|   0| 2.632781788454022|\n",
      "| 13|   0|0.5937630548774238|\n",
      "| 14|   0|0.5937630548774238|\n",
      "| 21|   0|2.8198594109249964|\n",
      "| 23|   0|0.6259355430408566|\n",
      "| 24|   0|0.6259355430408566|\n",
      "| 25|   0|1.2312494197128114|\n",
      "| 28|   0|0.6124584041875907|\n",
      "| 29|   0|0.6259355430408566|\n",
      "| 31|   0|1.0798740296508085|\n",
      "| 33|   0|1.0765750360500557|\n",
      "+---+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gmm_df.where(gmm_df['c_no'] == 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-----------------+\n",
      "|rid|           dist_c|           dist_c|\n",
      "+---+-----------------+-----------------+\n",
      "|425|5.353605956998742|5.353605956998742|\n",
      "|451|4.747783848259721|4.747783848259721|\n",
      "|457|5.993677023233796|5.993677023233796|\n",
      "+---+-----------------+-----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cols = ['rid', 'c_no', 'dist_c']\n",
    "gmm_df = rdd_w_clusts.toDF(cols)\n",
    "outlier_all = get_outliers.iqr_outliers(gmm_df.where(gmm_df['c_no'] == 0), 'dist_c')\n",
    "print(outlier_all.show())\n",
    "for i in range(1, 3):\n",
    "    outlier_c = get_outliers.iqr_outliers(gmm_df.where(gmm_df['c_no'] == i), 'dist_c')\n",
    "    outlier_all = outlier_all.unionAll(outlier_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+\n",
      "|rid|            dist_c|            dist_c|\n",
      "+---+------------------+------------------+\n",
      "|  5|0.8181668075756773|0.8181668075756773|\n",
      "| 58|0.4197031162934129|0.4197031162934129|\n",
      "| 93| 2.955095923872292| 2.955095923872292|\n",
      "|116|1.6243447061841065|1.6243447061841065|\n",
      "|159| 3.732245738548085| 3.732245738548085|\n",
      "|214|0.8811070218418926|0.8811070218418926|\n",
      "|215|0.8811070218418926|0.8811070218418926|\n",
      "|220| 2.522640762227058| 2.522640762227058|\n",
      "|228|0.9198827528155038|0.9198827528155038|\n",
      "|279| 2.125132118809308| 2.125132118809308|\n",
      "|285|1.1357407816111917|1.1357407816111917|\n",
      "|340| 2.361567562202725| 2.361567562202725|\n",
      "|348| 2.155426764956365| 2.155426764956365|\n",
      "|389|0.8942177110847539|0.8942177110847539|\n",
      "|420|1.2516598756087944|1.2516598756087944|\n",
      "|425|1.1725064364430433|1.1725064364430433|\n",
      "|428|1.2087188464618732|1.2087188464618732|\n",
      "|451|0.9531737413850345|0.9531737413850345|\n",
      "|452|3.5158242168139813|3.5158242168139813|\n",
      "|457|  1.48224191173903|  1.48224191173903|\n",
      "+---+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outlier_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rdd = labels.zip(zipped_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = []\n",
    "sigmas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    mus.append(np.array(gmm.gaussians[i].mu.toArray()).astype(float))\n",
    "    sigmas.append(np.array(gmm.gaussians[i].sigma.toArray()).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.44592339, 0.43449696]),\n",
       " array([-0.46776086, -0.35164155]),\n",
       " array([3.15045825, 1.74866176])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.47591458, 0.07829796],\n",
       "        [0.07829796, 0.74783396]]), array([[0.06576417, 0.03679376],\n",
       "        [0.03679376, 0.07849692]]), array([[ 2.13991533, -1.64727693],\n",
       "        [-1.64727693,  8.41836254]])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, DenseVector([3.2302, 0.1908])),\n",
       " (1, DenseVector([-0.8191, 0.402])),\n",
       " (2, DenseVector([0.3915, 1.683])),\n",
       " (3, DenseVector([0.3915, 1.683])),\n",
       " (4, DenseVector([0.3915, 1.683]))]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped_col.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_col = zipped_col.map(lambda x:x[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped_col.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.map(lambda x:x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = rid.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "col should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-b9e69af05341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.3.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \"\"\"\n\u001b[0;32m-> 1848\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: col should be Column"
     ]
    }
   ],
   "source": [
    "features.toDF().withColumn('rid',rid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
